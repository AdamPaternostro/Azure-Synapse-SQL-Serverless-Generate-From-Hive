{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spark Hive Table CodeGen to SQL OD\n",
        "- This will import two datasets from Azure Open Datasets. Taxi data and US Population by County\n",
        "- A Hive table will be created for each of the imported datasets.  Most customers overlay a Hive table on their data lake assets\n",
        "    - The taxi dataset will be partitioned by Year and Month so we can see how this is code generated for partitions\n",
        "    - The population dataset will not be partitioned\n",
        "    - The Hive table must also be created as \"STORED AS PARQUET\"\n",
        "- From the Hive table metadata a script will run that will code generate the SQL OD CREATE VIEW statements\n",
        "\n",
        "### Steps to run\n",
        "- Import this notebook into Synapse\n",
        "- In Cells 4, 5, 10 and 11 you need to change the abfs path \"abfss://\" to point to your storage\n",
        "\n",
        "### Creating the SQL OD views\n",
        "- Copy the generated code from Cell 13 and click on Develop hub and then create a new SQL Script.\n",
        "    - NOTE: You do not need to copy the first line \"dfTable: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [database: string, tableName: string ... 1 more field]\"\n",
        "- Paste the code\n",
        "- You should create a new SQL OD database ```CREATE DATABASE DataLake```  (You can name this what you like)\n",
        "- Run the code on a SQL OD database (DataLake) \n",
        "- Run the SELECT TOP 10 * statements to verify thing work\n",
        "- You can now call the SQL OD view from any tool that supports SQL Server (Excel, Java, Python, .NET, PowerBI, etc.)\n",
        "\n",
        "### Hive metastore sync\n",
        "- Also note, if you navigate to the Data hub and the click on Workspace and filter the databases to \"default (Spark)\" you will also see the tables.  You can technically query this table with SQL OD.  Your Hive metastore is kept in sync from Spark and SQL OD (only for tables that are STORED AS PARQUET).  The reason you might not want to use this metastore is that strings are all VARCHAR(MAX) and you have less control of the datatypes.  Using the smallest datatype possible is always best."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Taxi Data (Partitioned)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "puLocationId"
            ],
            "values": [
              "rateCodeID"
            ],
            "yLabel": "rateCodeID",
            "xLabel": "puLocationId",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "rateCodeID": {
              "": 1071
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "%%spark\n",
        "// Azure Open Datasets.  Read in NYC Green taxi data.\n",
        "val wasbs_path = \"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/green\"\n",
        "spark.conf.set(\"fs.azure.sas.nyctlc.azureopendatastorage.blob.core.windows.net\",\"\")\n",
        "val df = spark.read.parquet(wasbs_path)\n",
        "display(df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%spark\n",
        "// Write the data locally, this will save the data in the partitioned format\n",
        "df.write.partitionBy(\"puYear\", \"puMonth\").save(\"abfss://data-lake@paternostrosynapse.dfs.core.windows.net/nyx_taxi_green\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {},
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "-- Create an external table on your data lake.  Note, we store as PARQUET (Spark format)\n",
        "CREATE EXTERNAL TABLE nyx_taxi_green \n",
        "    (doLocationId STRING, dropoffLatitude DOUBLE, dropoffLongitude DOUBLE, extra DOUBLE, fareAmount DOUBLE, \n",
        "    improvementSurcharge STRING, lpepDropoffDatetime TIMESTAMP, lpepPickupDatetime TIMESTAMP, mtaTax DOUBLE,\n",
        "    passengerCount INT, paymentType INT, pickupLatitude DOUBLE, pickupLongitude DOUBLE, puLocationId STRING, \n",
        "    rateCodeID INT, storeAndFwdFlag STRING, tipAmount DOUBLE, tollsAmount DOUBLE, \n",
        "    totalAmount DOUBLE, tripDistance DOUBLE, tripType INT, vendorID INT)\n",
        "PARTITIONED BY (puYear INT, puMonth INT)\n",
        "STORED AS PARQUET\n",
        "LOCATION 'abfss://data-lake@paternostrosynapse.dfs.core.windows.net/nyx_taxi_green/'\n",
        "\n",
        "-- NOTE: \n",
        "-- If you get this error \"The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx-wx---;\", you need to grant \"other\" access to /tmp to correct.\n",
        "-- Open your data lake and find the folder /tmp and grant read/write/execute to other.  Set is as Access and Default.\n",
        "-- Do the same for the /tmp/hive folder."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {},
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "-- Since the data was not inserted into the talbe with an INSERT..INTO, Hive needs to refresh its partitions from storage\n",
        "MSCK REPAIR TABLE nyx_taxi_green"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "doLocationId"
            ],
            "values": [
              "dropoffLatitude"
            ],
            "yLabel": "dropoffLatitude",
            "xLabel": "doLocationId",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "dropoffLatitude": {
              "": 407.5659828186035
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "-- Make sure it works!\n",
        "SELECT *\n",
        "  FROM nyx_taxi_green\n",
        " LIMIT 10"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Population Data (Non-Partitioned)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "decennialTime"
            ],
            "values": [
              "population"
            ],
            "yLabel": "population",
            "xLabel": "decennialTime",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "population": {
              "2010": 667985
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "%%spark\n",
        "// Azure Open Datasets.  Read in NYC Green taxi data.\n",
        "val wasbs_path = \"wasbs://censusdatacontainer@azureopendatastorage.blob.core.windows.net/release/us_population_county/\"\n",
        "spark.conf.set(\"fs.azure.sas.censusdatacontainer.azureopendatastorage.blob.core.windows.net\",\"\")\n",
        "val df = spark.read.parquet(wasbs_path)\n",
        "display(df)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%spark\n",
        "// Write the data locally (no partitioning)\n",
        "df.write.save(\"abfss://data-lake@paternostrosynapse.dfs.core.windows.net/us_population_by_county\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {},
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "-- Create an external table on your data lake.  Note, we store as PARQUET (Spark format)\n",
        "CREATE EXTERNAL TABLE us_population_by_county \n",
        "    (decennialTime STRING, stateName STRING, countyName STRING, population INT, race STRING, sex STRING, minAge INT, maxAge INT, year INT)\n",
        "STORED AS PARQUET\n",
        "LOCATION 'abfss://data-lake@paternostrosynapse.dfs.core.windows.net/us_population_by_county/'\n",
        "\n",
        "-- NOTE: \n",
        "-- If you get this error \"The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwx-wx---;\", you need to grant \"other\" access to /tmp to correct.\n",
        "-- Open your data lake and find the folder /tmp and grant read/write/execute to other.  Set is as Access and Default.\n",
        "-- Do the same for the /tmp/hive folder."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "decennialTime"
            ],
            "values": [
              "population"
            ],
            "yLabel": "population",
            "xLabel": "decennialTime",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": {
            "population": {
              "2000": 8737
            }
          },
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": true
        }
      },
      "source": [
        "%%sql\n",
        "-- Make sure it works!\n",
        "SELECT *\n",
        "  FROM us_population_by_county\n",
        " LIMIT 10"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%spark\n",
        "\n",
        "val dfTable = spark.sql(\"SHOW TABLES\").filter(\"tableName == 'nyx_taxi_green' OR tableName == 'us_population_by_county'\") // you can run for all tables, but better to test with a few first\n",
        "  \n",
        "for (row <- dfTable.collect()) \n",
        "{   \n",
        "    var sqlOD = new StringBuilder(); \n",
        "    var sqlODPartitionSelect = new StringBuilder(); \n",
        "    var sqlODPartitionLocation = new StringBuilder(); \n",
        "    var sqlODRemovePartitionColFromTable = new StringBuilder(); \n",
        "    sqlOD.clear()\n",
        "    sqlODPartitionSelect.clear()\n",
        "    sqlODPartitionLocation.clear()\n",
        "    sqlODRemovePartitionColFromTable.clear()\n",
        "\n",
        "    var database = row.mkString(\",\").split(\",\")(0)\n",
        "    var tableName = row.mkString(\",\").split(\",\")(1)\n",
        "\n",
        "    val location = spark.sql(\"DESCRIBE EXTENDED \" + tableName).filter(\"col_name == 'Location'\").select($\"data_type\")\n",
        "    \n",
        "    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "    sqlOD.append(\"-- \" + tableName + \"\\n\")\n",
        "    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "    sqlOD.append(\"IF EXISTS (SELECT * FROM sys.objects WHERE name ='\" + tableName + \"')\\n\")\n",
        "    sqlOD.append(\"  BEGIN\\n\")\n",
        "    sqlOD.append(\"  DROP VIEW \" + tableName + \";\\n\")\n",
        "    sqlOD.append(\"  END\\n\")\n",
        "    sqlOD.append(\"GO\\n\\n\")\n",
        "    sqlOD.append(\"CREATE VIEW \" + tableName + \" AS\\n\")\n",
        "    sqlOD.append(\"SELECT\\n\")\n",
        "    sqlOD.append(\"REPLACE_ME_sqlODPartitionSelect\")\n",
        "    sqlOD.append(\"  *\\n\")\n",
        "    sqlOD.append(\"FROM OPENROWSET(\\n\")\n",
        "    sqlOD.append(\"   BULK '\")\n",
        "    for (loc <- location.collect())\n",
        "    {\n",
        "        var abfsPath = loc.mkString(\",\").split(\",\")(0)\n",
        "        sqlOD.append(abfsPath)\n",
        "        sqlOD.append(\"REPLACE_ME_sqlODPartitionLocation\")\n",
        "    }\n",
        "    sqlOD.append(\"/*.parquet',\\n\")\n",
        "    sqlOD.append(\"   FORMAT='PARQUET'\\n\")\n",
        "    sqlOD.append(\")\\n\")\n",
        "    sqlOD.append(\"WITH (\\n\")\n",
        "\n",
        "    val cols = spark.sql(\"DESCRIBE \" + tableName)\n",
        "    var i = 1;\n",
        "    var partitionIndex = 1;\n",
        "    var numberOfColumns = cols.collect().length\n",
        "    var processingPartitions = false\n",
        "\n",
        "    for (col <- cols.collect())\n",
        "    {\n",
        "        var colString = col.mkString(\"|\")\n",
        "        //println(colString)\n",
        "        var colSplit = colString.split('|')\n",
        "        //colSplit.foreach(println) \n",
        "        var col_name = colSplit(0)\n",
        "        var data_type = \"\"\n",
        "        try\n",
        "        {\n",
        "            data_type = colSplit(1)\n",
        "        }\n",
        "        catch {\n",
        "            case e: Exception => var void = \"\"\n",
        "        }\n",
        "\n",
        "        // Map data types (MORE WORK needs to be done here, this is just a few mappings)\n",
        "        if (data_type == \"string\")\n",
        "        {\n",
        "            data_type = \"VARCHAR(255)\"    // NOTE: This is just a fixed number that was picked since it covered most of the string sizes.  \n",
        "        }\n",
        "        if (data_type == \"double\")\n",
        "        {\n",
        "            data_type = \"FLOAT\"\n",
        "        }\n",
        "         if (data_type == \"timestamp\")\n",
        "        {\n",
        "            data_type = \"DATETIME2\"\n",
        "        }      \n",
        "\n",
        "        if (col_name == \"# Partition Information\" || col_name== \"# col_name\")\n",
        "        {\n",
        "            processingPartitions = true;\n",
        "        }\n",
        "\n",
        "        if (processingPartitions == true && col_name != \"# Partition Information\" && col_name != \"# col_name\")\n",
        "        {\n",
        "            sqlODPartitionSelect.append(\"   CAST(r.filepath(\" + partitionIndex.toString() + \") AS \" + data_type + \") AS \" + col_name + \",\\n\")\n",
        "            sqlODPartitionLocation.append(\"/\" + col_name + \"=*\")\n",
        "            partitionIndex = partitionIndex + 1 \n",
        "            sqlODRemovePartitionColFromTable.append(\"  \" + col_name + \" \" + data_type).append(\",\\n\")\n",
        "       }\n",
        "        \n",
        "        if (processingPartitions == false)\n",
        "        {\n",
        "            sqlOD.append(\"  \" + col_name + \" \" + data_type).append(\",\\n\")\n",
        "        }\n",
        "        i = i + 1\n",
        "    }\n",
        "          \n",
        "    sqlOD.append(\") AS [r];\\n\")\n",
        "    sqlOD.append(\"GO\\n\\n\")\n",
        "    sqlOD.append(\"-- SELECT TOP 10 * FROM \" + tableName + \";\\n\")\n",
        "    sqlOD.append(\"-----------------------------------------------------------------------------------------------\\n\")\n",
        "    sqlOD.append(\"\\n\")\n",
        "    sqlOD.append(\"\\n\")\n",
        "\n",
        "    println(sqlOD.toString()\n",
        "    .replace(\"REPLACE_ME_sqlODPartitionSelect\",sqlODPartitionSelect.toString())\n",
        "    .replace(\"REPLACE_ME_sqlODPartitionLocation\",sqlODPartitionLocation.toString())\n",
        "    .replace(sqlODRemovePartitionColFromTable.toString(),\"\") // remove partition columns from WITH statement\n",
        "    .replace(\",\\n) AS [r];\",\"\\n) AS [r];\") // remove the trailing comma from the WITH statement\n",
        "    )\n",
        "}"
      ],
      "attachments": {}
    }
  ]
}